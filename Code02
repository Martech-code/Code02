import time
from datetime import datetime
import random
import json
import csv
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchWindowException
from collections import defaultdict, deque
from typing import Dict, List
from selenium.webdriver.common.action_chains import ActionChains
from datetime import datetime, timedelta
import os
import requests
import threading
import math
import sys

# Configuration parameters
MIN_DELAY_BETWEEN_REQUESTS = 0.5
MAX_DELAY_BETWEEN_REQUESTS = 1
MAX_RETRIES = 3
PAGE_LOAD_TIMEOUT = 10
TAB_SWITCH_DELAY = 0.2
MAX_TABS = 5
SCROLL_PAUSE_TIME = 2
# TELEGRAM_TOKEN = "7584627793:AAHPB4zaMSTYgu2P6N9RMlplr0KcFMP1FQQ"
# TELEGRAM_CHAT_ID = "-1002371940283"

def calculate_std_dev(values):
    n = len(values)
    if n < 2:
        return 0
    mean = sum(values) / n
    squared_diff_sum = sum((x - mean) ** 2 for x in values)
    return math.sqrt(squared_diff_sum / (n - 1))

class ChromeInstance:
    def __init__(self, debug_port, worker_id):
        self.debug_port = debug_port
        self.worker_id = worker_id
        self.driver = None
        self.processed_count = 0
        self.is_checkpoint = False
        self.is_blocked = False
        self.processed_groups = set()
        self.collected_urls = []
        self.failed_urls = []
        self.processed_urls = set()  # Thêm tracking URLs đã xử lý
        self.skipped_urls = []      # Thêm tracking URLs bị bỏ qua
        
    def track_processed_url(self, url):
        self.processed_urls.add(url)
        
    def track_skipped_url(self, url, reason):
        self.skipped_urls.append({
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'reason': reason
        })

    def save_progress(self, keyword, urls, current_index, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        progress_data = {
            'keyword': keyword,
            'urls': urls,
            'current_index': current_index,
            'timestamp': datetime.now().isoformat()
        }
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi lưu tiến độ: {str(e)}")

    def load_progress(self, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        try:
            if os.path.exists(progress_file):
                with open(progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi đọc tiến độ: {str(e)}")
        return None

    def clear_progress(self, output_dir):
        progress_file = os.path.join(output_dir, f'progress_worker_{self.worker_id}.json')
        try:
            if os.path.exists(progress_file):
                os.remove(progress_file)
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi xóa file tiến độ: {str(e)}")

    def setup_chrome_driver(self):
        chrome_options = Options()
        chrome_options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.debug_port}")
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-notifications')
        chrome_options.add_argument('--disable-infobars')
        chrome_options.add_argument('--lang=vi')
        return chrome_options

    def create_driver(self):
        try:
            options = self.setup_chrome_driver()
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
            return self.driver
        except Exception as e:
            raise Exception(f"Lỗi khởi tạo Chrome WebDriver port {self.debug_port}: {str(e)}")

    def close_block_popup(self):
        try:
            popup = self.driver.find_element(By.XPATH, "//div[contains(@class, 'x1ey2m1c') and contains(@class, 'xds687c') and contains(@class, 'x17qophe') and contains(@class, 'xg01cxk') and contains(@class, 'x47corl') and contains(@class, 'x10l6tqk') and contains(@class, 'x13vifvy') and contains(@class, 'x1ebt8du') and contains(@class, 'x19991ni') and contains(@class, 'x1dhq9h') and contains(@class, 'xzolkzo') and contains(@class, 'x12go9s9') and contains(@class, 'x1rnf11y') and contains(@class, 'xprq8jg')]")
            actions = ActionChains(self.driver)
            actions.double_click(popup).perform()
            time.sleep(1)
            return True
        except:
            return False

    def check_checkpoint(self):
        try:
            current_url = self.driver.current_url.lower()
            checkpoint_indicators = [
                "facebook.com/checkpoint",
                "facebook.com/recover",
                "facebook.com/login",
                "/login/?next",
                "checkpoint/?next"
            ]
            return any(indicator in current_url for indicator in checkpoint_indicators)
        except:
            return False

    def check_blocked(self):
        try:
            blocked_text = "Bạn tạm thời bị chặn"
            blocked_text_2 = "Có vẻ như bạn đang dùng nhầm tính năng này do sử dụng quá nhanh"
            page_source = self.driver.page_source
            return blocked_text in page_source or blocked_text_2 in page_source
        except:
            return False

    def check_suspicious(self):
        try:
            # Theo dõi URL hiện tại
            current_url = self.driver.current_url.lower()
            
            # Nếu URL chứa checkpoint suspicious
            if "facebook.com/checkpoint/601051028565049" in current_url:
                # Tìm nút "Bỏ qua" hoặc "Dismiss"
                skip_button = self.driver.find_elements(By.XPATH, "//span[contains(@class, 'x1lliihq') and contains(@class, 'x6ikm8r') and contains(@class, 'x10wlt62') and contains(@class, 'x1n2onr6') and contains(@class, 'xlyipyv') and contains(@class, 'xuxw1ft') and (contains(text(), 'Bỏ qua') or contains(text(), 'Dismiss'))]")
                
                if skip_button:
                    try:
                        actions = ActionChains(self.driver)
                        actions.double_click(skip_button[0]).perform()
                        print(f"[Worker {self.worker_id}] Đã click nút Bỏ qua trên trang Checkpoint")
                        time.sleep(1)
                        return True
                    except Exception as e:
                        print(f"[Worker {self.worker_id}] Lỗi khi click nút Bỏ qua: {str(e)}")
                        return False
            return False
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi khi kiểm tra trang suspicious: {str(e)}")
            return False

    # Thêm hàm mới để theo dõi URL
    def url_monitor(self):
        """Hàm theo dõi URL và xử lý checkpoint"""
        try:
            current_url = self.driver.current_url
            if "facebook.com/checkpoint/601051028565049" in current_url.lower():
                self.check_suspicious()
        except Exception as e:
            print(f"[Worker {self.worker_id}] Lỗi khi theo dõi URL: {str(e)}")

    def scroll_and_collect_groups(self):
        print(f"[Worker {self.worker_id}] Bắt đầu scroll và thu thập URLs...")
        group_links = []  # Đổi từ set sang list để duy trì thứ tự
        group_links_set = set()  # Dùng set để kiểm tra trùng lặp
        no_new_content_count = 0
        previous_unique_count = 0
        last_height = self.driver.execute_script("return document.documentElement.scrollHeight")
        
        while True:
            current_height = self.driver.execute_script("return window.pageYOffset")
            target_height = min(current_height + 300, last_height)
            self.driver.execute_script(f"window.scrollTo(0, {target_height});")
            time.sleep(0.3)

            time.sleep(SCROLL_PAUSE_TIME)
            
            actual_height = self.driver.execute_script("return window.pageYOffset")
            if abs(actual_height - target_height) > 50:
                self.driver.execute_script(f"window.scrollTo(0, {target_height});")
                time.sleep(0.3)

            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/groups/']")
            
            new_urls_added = 0
            for element in elements:
                try:
                    href = element.get_attribute('href')
                    if href and re.search(r'facebook\.com/groups/\d+/?$', href):
                        clean_url = re.sub(r'\?.*$', '', href)
                        if clean_url not in self.processed_groups and clean_url not in group_links_set:
                            group_links.append(clean_url)  # Thêm vào list để giữ thứ tự
                            group_links_set.add(clean_url)  # Thêm vào set để kiểm tra trùng lặp
                            new_urls_added += 1
                except:
                    continue
            
            current_unique_count = len(group_links)
            print(f"[Worker {self.worker_id}] Đã tìm thấy {new_urls_added} nhóm mới (Tổng: {current_unique_count} nhóm độc nhất)")
            
            if current_unique_count == previous_unique_count:
                no_new_content_count += 1
                print(f"[Worker {self.worker_id}] Không tìm thấy nhóm mới... ({no_new_content_count}/8)")
                
                # Kiểm tra popup khi no_new_content_count = 6
                if no_new_content_count == 6:
                    print(f"[Worker {self.worker_id}] Kiểm tra và xử lý popup tại lần thứ 6...")
                    if self.close_block_popup():
                        print(f"[Worker {self.worker_id}] Đã đóng popup block, tiếp tục thu thập...")
                        time.sleep(1)
                        continue  # Bỏ qua phần còn lại và tiếp tục vòng lặp
                
                if no_new_content_count >= 8:
                    print(f"[Worker {self.worker_id}] Hoàn tất thu thập URLs sau 8 lần không có nhóm mới")
                    break
            else:
                no_new_content_count = 0
                previous_unique_count = current_unique_count

            new_height = self.driver.execute_script("return document.documentElement.scrollHeight")
            if new_height != last_height:
                last_height = new_height
                no_new_content_count = 0
            elif abs(actual_height - last_height) < 100:
                time.sleep(SCROLL_PAUSE_TIME * 2)
                if abs(actual_height - self.driver.execute_script("return document.documentElement.scrollHeight")) < 100:
                    print(f"[Worker {self.worker_id}] Đã đến cuối trang")
                    break

        print(f"[Worker {self.worker_id}] Tổng cộng đã thu thập được {len(group_links)} URLs độc nhất")
        return group_links  # Trả về list thay vì set

def save_group_urls(urls, keyword, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'collected_urls_worker_{worker_id}.json')
    data = {
        'keyword': keyword,
        'timestamp': datetime.now().isoformat(),
        'urls': urls,
        'total_urls': len(urls)
    }

    try:
        existing_data = []
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                try:
                    existing_data = json.load(f)
                    if not isinstance(existing_data, list):
                        existing_data = [existing_data]
                except json.JSONDecodeError:
                    existing_data = []
        
        # Tạo set của URLs đã tồn tại
        existing_urls = set()
        for entry in existing_data:
            existing_urls.update(entry.get('urls', []))
        
        # Lọc URLs mới và giữ nguyên thứ tự
        new_urls = [url for url in urls if url not in existing_urls]
        if new_urls:
            data['urls'] = new_urls
            data['total_urls'] = len(new_urls)
            existing_data.append(data)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            print(f"[Worker {worker_id}] Đã lưu {len(new_urls)} URLs mới vào file")
        else:
            print(f"[Worker {worker_id}] Không có URLs mới để lưu")
            
    except Exception as e:
        print(f"Error saving URLs to JSON: {str(e)}")

def save_failed_urls(failed_urls, output_dir, worker_id):
    file_path = os.path.join(output_dir, f'failed_urls_worker_{worker_id}.json')
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump({
                'failed_urls': failed_urls,
                'total_failed': len(failed_urls),
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        print(f"[Worker {worker_id}] Đã lưu {len(failed_urls)} URLs thất bại vào file")
    except Exception as e:
        print(f"Error saving failed URLs: {str(e)}")

def convert_members_count(count_text):
    try:
        # Remove "thành viên", "members" text and whitespace
        count_text = count_text.lower().replace('thành viên', '').replace('members', '').replace(' ', '').strip()
        
        # Standardize decimal separator to dot
        if ',' in count_text:
            count_text = count_text.replace(',', '.')
        
        # Handle "triệu" (Vietnamese millions)
        if 'triệu' in count_text:
            number = float(count_text.replace('triệu', ''))
            return int(number * 1000000)
        
        # Handle K (thousands)
        if 'k' in count_text:
            number = float(count_text.replace('k', ''))
            return int(number * 1000)
        
        # Handle M (millions) 
        if 'm' in count_text:
            number = float(count_text.replace('m', ''))
            return int(number * 1000000)
            
        # Handle B (billions)
        if 'b' in count_text:
            number = float(count_text.replace('b', ''))
            return int(number * 1000000000)
            
        # Handle regular numbers
        return int(float(count_text))
    except:
        return 0

def process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count=0):
    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm: {group_url}")
    try:
        chrome_instance.driver.get(group_url)
        chrome_instance.url_monitor()
        
        time.sleep(random.uniform(2, 3))

        def handle_popup():
            """Xử lý popup block"""
            if chrome_instance.close_block_popup():
                print(f"[Worker {chrome_instance.worker_id}] Đã đóng popup block")
                time.sleep(random.uniform(1, 2))
                return True
            return False

        # Check popup sau khi load trang
        time.sleep(1)
        handle_popup()

        if "content not found" in chrome_instance.driver.page_source.lower() or "không tìm thấy nội dung" in chrome_instance.driver.page_source.lower():
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 3))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể tải trang sau nhiều lần thử")

        try:
            group_name_element = WebDriverWait(chrome_instance.driver, 2).until(
                EC.presence_of_element_located((By.XPATH, "//h1//a[contains(@class, 'x16tdsg8')]"))
            )
            group_name = group_name_element.text.strip()
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                raise Exception("Không thể lấy tên nhóm")

        try:
            group_type_element = WebDriverWait(chrome_instance.driver, 1).until(
                EC.presence_of_element_located((By.XPATH, "//div[contains(text(), 'Nhóm') and contains(@class, 'x1n2onr6')]"))
            )
            group_type = group_type_element.text.strip()
        except:
            group_type = "Unknown"

        try:
            members_element = WebDriverWait(chrome_instance.driver, 1).until(
                EC.presence_of_element_located((By.XPATH, "//a[contains(@href, '/members/')]"))
            )
            members_text = members_element.text
            members_match = re.search(r'([\d,\.]+[KMB]?) thành viên', members_text)
            members_count = members_match.group(1) if members_match else "0"
        except:
            if retry_count < MAX_RETRIES:
                time.sleep(random.uniform(1, 2))
                return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
            else:
                members_count = "0"

        numeric_members_count = convert_members_count(members_count)

        # Kiểm tra nếu không lấy được group_type và members_count
        if group_type == "Unknown" and members_count == "0":
            chrome_instance.failed_urls.append({
                'url': group_url,
                'error': "Không thể lấy group_type và members_count",
                'timestamp': datetime.now().isoformat()
            })
            return True

        try:
            can_post = "No"
            try:
                post_box_vn = WebDriverWait(chrome_instance.driver, 2).until(
                    EC.presence_of_element_located((By.XPATH, '//span[@class="x1lliihq x6ikm8r x10wlt62 x1n2onr6" and contains(text(), "Bạn viết gì đi...")]'))
                )
                can_post = "Yes"
            except:
                try:
                    post_box_en = WebDriverWait(chrome_instance.driver, 2).until(
                        EC.presence_of_element_located((By.XPATH, '//span[@class="x1lliihq x6ikm8r x10wlt62 x1n2onr6" and contains(text(), "Write something...")]'))
                    )
                    can_post = "Yes"
                except:
                    can_post = "No"
        except Exception as e:
            print(f"[Worker {chrome_instance.worker_id}] Lỗi khi kiểm tra quyền đăng bài: {str(e)}")
            can_post = "No"

        # Thực hiện scroll
        num_scrolls = random.randint(1, 3)
        for i in range(num_scrolls):
            chrome_instance.driver.execute_script("window.scrollBy(0, 600)")
            time.sleep(random.uniform(0.5, 1))
            
            # Check popup sau lần scroll cuối cùng
            if i == num_scrolls - 1:  # Thay đổi điều kiện từ i == 0 thành i == num_scrolls - 1
                time.sleep(1)
                handle_popup()

        group_data = {
            'keyword_search': search_keywords,
            'group_name': group_name,
            'group_link': group_url,
            'group_type': group_type,
            'members_count': numeric_members_count,
            'can_post': can_post
        }

        with lock:
            try:
                write_header = not os.path.exists(output_file)
                with open(output_file, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=list(group_data.keys()))
                    if write_header:
                        writer.writeheader()
                    writer.writerow(group_data)
                print(f"[Worker {chrome_instance.worker_id}] Đã lưu thành công nhóm: {group_name}")
                return True
            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi lưu dữ liệu: {str(e)}")
                return False

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý nhóm {group_url}: {str(e)}")
        chrome_instance.failed_urls.append({
            'url': group_url,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        })
        if retry_count < MAX_RETRIES:
            time.sleep(random.uniform(1, 2))
            return process_single_group(chrome_instance, group_url, search_keywords, output_file, lock, retry_count + 1)
        return False

def worker_process(chrome_instance: ChromeInstance, keywords: List[str], processed_items: set, processed_groups: set, lock, output_dir):
    try:
        driver = chrome_instance.create_driver()
        output_file = os.path.join(output_dir, 'groups_data.csv')
        retry_count = 0

        # Kiểm tra và load tiến độ trước đó
        progress_data = chrome_instance.load_progress(output_dir)
        if progress_data:
            print(f"[Worker {chrome_instance.worker_id}] Tìm thấy tiến độ trước đó:")
            print(f"- Keyword: {progress_data['keyword']}")
            print(f"- Số URL còn lại: {len(progress_data['urls']) - progress_data['current_index']}")
            
            # Thêm tracking URLs đã xử lý
            processed_urls_tracking = set()
            skipped_urls = []
            
            remaining_urls = progress_data['urls'][progress_data['current_index']:]
            total_remaining = len(remaining_urls)
            
            for idx, group_url in enumerate(remaining_urls, 1):
                current_index = progress_data['current_index'] + idx - 1
                
                # Kiểm tra URL đã xử lý chưa
                if group_url in processed_groups or group_url in processed_urls_tracking:
                    print(f"[Worker {chrome_instance.worker_id}] URL đã xử lý trước đó: {group_url}")
                    continue

                print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm {idx}/{total_remaining} (Index: {current_index})")
                
                try:
                    if process_single_group(chrome_instance, group_url, progress_data['keyword'], output_file, lock):
                        with lock:
                            processed_groups.add(group_url)
                            chrome_instance.processed_groups.add(group_url)
                            processed_urls_tracking.add(group_url)
                    else:
                        skipped_urls.append({
                            'url': group_url,
                            'index': current_index,
                            'reason': 'Processing failed'
                        })
                except Exception as e:
                    print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý URL {group_url}: {str(e)}")
                    skipped_urls.append({
                        'url': group_url,
                        'index': current_index,
                        'reason': str(e)
                    })

                # Lưu tiến độ sau mỗi URL
                chrome_instance.save_progress(
                    progress_data['keyword'],
                    progress_data['urls'],
                    current_index + 1,  # Cập nhật index chính xác
                    output_dir
                )

                time.sleep(random.uniform(MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS))

            # Lưu thông tin URLs bị bỏ qua
            if skipped_urls:
                skipped_file = os.path.join(output_dir, f'skipped_urls_worker_{chrome_instance.worker_id}.json')
                with open(skipped_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'keyword': progress_data['keyword'],
                        'skipped_urls': skipped_urls,
                        'timestamp': datetime.now().isoformat()
                    }, f, ensure_ascii=False, indent=2)
                print(f"[Worker {chrome_instance.worker_id}] Đã lưu {len(skipped_urls)} URLs bị bỏ qua")

            # Xóa file tiến độ sau khi hoàn thành
            chrome_instance.clear_progress(output_dir)
            print(f"[Worker {chrome_instance.worker_id}] Đã hoàn thành xử lý các URL còn lại")

        while True:
            if chrome_instance.check_checkpoint():
                chrome_instance.is_checkpoint = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị checkpoint, dừng worker")
                break

            if chrome_instance.check_blocked():
                chrome_instance.is_blocked = True
                print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị chặn tạm thời, dừng worker")
                break
            
            if chrome_instance.check_suspicious():
                print(f"[Worker {chrome_instance.worker_id}] Đã xử lý cảnh báo suspicious")
                time.sleep(random.uniform(1, 2))
                continue

            with lock:
                if len(keywords) < 2:
                    break
                selected_keywords = random.sample(keywords, 2)
                search_keywords = ' '.join(selected_keywords)

            if search_keywords in processed_items:
                continue

            try:
                encoded_keywords = '%20'.join(search_keywords.split())
                search_url = f"https://www.facebook.com/groups/search/groups?q={encoded_keywords}&filters=eyJwdWJsaWNfZ3JvdXBzOjAiOiJ7XCJuYW1lXCI6XCJwdWJsaWNfZ3JvdXBzXCIsXCJhcmdzXCI6XCJcIn0ifQ%3D%3D"

                print(f"\n[Worker {chrome_instance.worker_id}] Tìm kiếm với từ khóa: {search_keywords}")
                driver.get(search_url)
                time.sleep(random.uniform(2, 3))

                group_urls = chrome_instance.scroll_and_collect_groups()
                if not group_urls:
                    print(f"[Worker {chrome_instance.worker_id}] Không tìm thấy URLs cho từ khóa: {search_keywords}")
                    continue

                save_group_urls(group_urls, search_keywords, output_dir, chrome_instance.worker_id)

                total_urls = len(group_urls)
                processed_urls = 0
                failed_urls = 0

                print(f"[Worker {chrome_instance.worker_id}] Bắt đầu xử lý {total_urls} nhóm...")
                
                for idx, group_url in enumerate(group_urls, 1):
                    if group_url in processed_groups:
                        processed_urls += 1
                        continue

                    print(f"[Worker {chrome_instance.worker_id}] Đang xử lý nhóm {idx}/{total_urls}")
                    
                    # Lưu tiến độ trước khi xử lý mỗi URL
                    chrome_instance.save_progress(
                        search_keywords,
                        group_urls,
                        idx - 1,
                        output_dir
                    )
                    
                    if process_single_group(chrome_instance, group_url, search_keywords, output_file, lock):
                        with lock:
                            processed_groups.add(group_url)
                            chrome_instance.processed_groups.add(group_url)
                        processed_urls += 1
                    else:
                        failed_urls += 1

                    time.sleep(random.uniform(MIN_DELAY_BETWEEN_REQUESTS, MAX_DELAY_BETWEEN_REQUESTS))

                # Xóa file tiến độ sau khi hoàn thành keyword
                chrome_instance.clear_progress(output_dir)

                print(f"[Worker {chrome_instance.worker_id}] Kết quả xử lý keyword '{search_keywords}':")
                print(f"- Tổng số URLs: {total_urls}")
                print(f"- Đã xử lý: {processed_urls}")
                print(f"- Thất bại: {failed_urls}")
                
                chrome_instance.close_block_popup()
                time.sleep(1)  # Chờ 1 giây sau khi đóng popup

                if chrome_instance.check_blocked():
                    chrome_instance.is_blocked = True
                    print(f"[Worker {chrome_instance.worker_id}] Tài khoản bị chặn tạm thời, dừng worker")
                    break                

                chrome_instance.processed_count += 1
                with lock:
                    processed_items.add(search_keywords)
                    if chrome_instance.processed_count % 10 == 0:
                        save_progress(processed_items, processed_groups, output_dir)

            except Exception as e:
                print(f"[Worker {chrome_instance.worker_id}] Lỗi xử lý từ khóa {search_keywords}: {str(e)}")
                retry_count += 1
                if retry_count >= MAX_RETRIES:
                    print(f"[Worker {chrome_instance.worker_id}] Đã vượt quá số lần thử lại cho phép")
                    break
                time.sleep(random.uniform(2, 4))

    except Exception as e:
        print(f"[Worker {chrome_instance.worker_id}] Lỗi: {str(e)}")
    finally:
        if chrome_instance.failed_urls:
            save_failed_urls(chrome_instance.failed_urls, output_dir, chrome_instance.worker_id)
        if chrome_instance.driver:
            chrome_instance.driver.quit()

def load_keywords(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def get_combinations_count(n):
    # Tính số tổ hợp chập 2 của n phần tử
    return (n * (n - 1)) // 2

def load_progress(output_dir):
    try:
        with open(os.path.join(output_dir, 'scan_progress.json'), 'r') as f:
            data = json.load(f)
            return data.get('processed_items', []), set(data.get('processed_groups', []))
    except:
        return [], set()

def save_progress(processed_items, processed_groups, output_dir):
    with open(os.path.join(output_dir, 'scan_progress.json'), 'w') as f:
        json.dump({
            'processed_items': list(processed_items),
            'processed_groups': list(processed_groups)
        }, f)

class OutputLogger:
    def __init__(self, output_dir):
        self.terminal = sys.stdout
        self.log_file = os.path.join(output_dir, 'console_log.json')
        self.log_data = []
        self.MAX_LOG_ENTRIES = 1000
        
        # Load existing logs if any
        if os.path.exists(self.log_file):
            try:
                with open(self.log_file, 'r', encoding='utf-8') as f:
                    self.log_data = json.load(f)
                    # Ensure we only keep the latest MAX_LOG_ENTRIES
                    if len(self.log_data) > self.MAX_LOG_ENTRIES:
                        self.log_data = self.log_data[-self.MAX_LOG_ENTRIES:]
            except:
                self.log_data = []

    def write(self, message):
        self.terminal.write(message)
        if message.strip():  # Only log non-empty messages
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'message': message.strip()
            }
            # Add new entry at the beginning of the list
            self.log_data.insert(0, log_entry)
            
            # Keep only the latest MAX_LOG_ENTRIES
            if len(self.log_data) > self.MAX_LOG_ENTRIES:
                self.log_data = self.log_data[:self.MAX_LOG_ENTRIES]
            
            # Save to file
            try:
                with open(self.log_file, 'w', encoding='utf-8') as f:
                    json.dump(self.log_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                self.terminal.write(f"\nError saving log: {str(e)}\n")

    def flush(self):
        self.terminal.flush()

# def send_telegram_message(bot_token, chat_id, message):
    # try:
        # url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        # data = {
            # "chat_id": chat_id,
            # "text": message,
            # "parse_mode": "HTML"
        # }
        # response = requests.post(url, json=data)
        # response.raise_for_status()
    # except Exception as e:
        # print(f"Error sending Telegram message: {str(e)}")

def count_active_workers(log_file):
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            logs = json.load(f)
            
        # Get current time
        current_time = datetime.now()
        
        # Look at logs from the last 5 minutes
        recent_time = current_time - timedelta(minutes=5)
        
        # Get unique active workers from recent logs
        active_workers = set()
        error_workers = set()
        
        for log in logs:
            log_time = datetime.fromisoformat(log['timestamp'])
            if log_time >= recent_time:
                # Check for worker numbers in messages
                worker_match = re.search(r'\[Worker (\d+)\]', log['message'])
                if worker_match:
                    worker_num = worker_match.group(1)
                    # Check if the message indicates an error or account blocked
                    if ('error' in log['message'].lower() or 
                        'lỗi' in log['message'].lower() or 
                        'tài khoản bị chặn tạm thời' in log['message'].lower()):
                        error_workers.add(worker_num)
                    else:
                        active_workers.add(worker_num)
        
        # Remove error workers from active workers
        active_workers = active_workers - error_workers
        
        return len(active_workers)
    except Exception as e:
        print(f"Error counting active workers: {str(e)}")
        return 0

# def monitor_workers(output_dir, bot_token, chat_id):
    # log_file = os.path.join(output_dir, 'console_log.json')
    
    # Wait for initial delay (1 minute)
    # time.sleep(60)
    
    # while True:
        # try:
            # active_count = count_active_workers(log_file)
            # current_time = datetime.now().strftime("%b %d")
            # message = f"May 15: Hiện tại đang có {active_count} Worker làm việc"
            # send_telegram_message(bot_token, chat_id, message)
            
            # Wait for 2H before next check
            # time.sleep(7200)
        # except Exception as e:
            # print(f"Error in monitor_workers: {str(e)}")
            # time.sleep(60)  # Wait 1 minute before retrying if there's an error

def main():
    input_file = r'C:\Users\Admin-DL\Downloads\Tìm Group Công Khai\Keyword.txt'
    output_dir = r'C:\Users\Admin-DL\Downloads\Tìm Group Công Khai\output'

    os.makedirs(output_dir, exist_ok=True)
    
    # Add this line to capture all console output
    sys.stdout = OutputLogger(output_dir)
    
    # Start worker monitoring in a separate thread
    # monitor_thread = threading.Thread(
        # target=monitor_workers,
        # args=(output_dir, TELEGRAM_TOKEN, TELEGRAM_CHAT_ID)
    # )
    # monitor_thread.daemon = True
    # monitor_thread.start()

    keywords = load_keywords(input_file)
    processed_items, processed_groups = load_progress(output_dir)
    processed_items = set(processed_items)

    total_keywords = len(keywords)
    total_combinations = get_combinations_count(total_keywords)
    used_combinations = len(processed_items)

    print(f"Tổng số keywords gốc: {total_keywords}")
    print(f"Tổng số tổ hợp có thể: {total_combinations}")
    print(f"Số tổ hợp đã sử dụng: {used_combinations}")
    print(f"Còn lại: {total_combinations - used_combinations} tổ hợp chưa sử dụng")

    chrome_instances = [
        ChromeInstance(debug_port=9222, worker_id=1),
        ChromeInstance(debug_port=9223, worker_id=2),
        ChromeInstance(debug_port=9224, worker_id=3),
        ChromeInstance(debug_port=9225, worker_id=4),
        ChromeInstance(debug_port=9226, worker_id=5),
        ChromeInstance(debug_port=9227, worker_id=6),
        ChromeInstance(debug_port=9228, worker_id=7),
        ChromeInstance(debug_port=9229, worker_id=8),
        ChromeInstance(debug_port=9230, worker_id=9),
        ChromeInstance(debug_port=9231, worker_id=10)
    ]

    lock = threading.Lock()

    threads = []
    for chrome_instance in chrome_instances:
        thread = threading.Thread(
            target=worker_process,
            args=(chrome_instance, keywords, processed_items, processed_groups, lock, output_dir)
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    print("\nHoàn tất xử lý!")

if __name__ == "__main__":
    main()
